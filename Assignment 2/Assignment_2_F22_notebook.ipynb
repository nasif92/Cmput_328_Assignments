{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfn-Yiy1lp5l"
      },
      "source": [
        "# Lab Assignment 2 \n",
        "## With this assignment you will get to know more about gradient descent optimization and writing your own functions with forward and backward (i.e., gradient) passes\n",
        "## You need to complete all the tasks in this notebook in the lab. Edit only those portions in the cells where it asks you to do so!\n",
        "## Submit **only the notebook file** for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXQy7P_5y0DR"
      },
      "source": [
        "### Please Fill these information.\n",
        "##**Your Name**: Nasif Hossain\n",
        "##**Your CCID**: 1545143"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Zp3BetP-d6cB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3fhaBmte8ch"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJpovSL8d_-l"
      },
      "source": [
        "## Huber loss function\n",
        "https://en.wikipedia.org/wiki/Huber_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GTp4nNf9d-zg"
      },
      "outputs": [],
      "source": [
        "# A loss function measures distance between a predicted and a target tensor\n",
        "# An implementation of Huber loss function is given below\n",
        "# We will make use of this loss function in gradient descent optimization\n",
        "def Huber_Loss(input,delta):\n",
        "  m = (torch.abs(input)<=delta).detach().float()\n",
        "  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoxXPadgk-O"
      },
      "source": [
        "# Test Huber loss with a couple of different examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYO_KmUQfmnm",
        "outputId": "347b2b1b-72a3-4a9f-fab3-b5d5bac7900a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.3  2.  -3.1]\n",
            " [ 0.5  9.2  0.1]]\n",
            "12.975\n",
            "[0.3 2. ]\n",
            "1.545\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n",
        "print(a.numpy())\n",
        "ha = Huber_Loss(a,1.0)\n",
        "print(ha.numpy())\n",
        "\n",
        "b = torch.tensor([0.3, 2.0])\n",
        "print(b.numpy())\n",
        "hb = Huber_Loss(b,1.0)\n",
        "print(hb.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26wLACTj7FkG"
      },
      "source": [
        "# Gradient descent code\n",
        "## Study the following generic gradient descent optimization code.\n",
        "## Huber loss f measures the distance between a probability vector `z` and target 1-hot vector `target`.\n",
        "## When `f.backward` is called, PyTorch first computes $\\nabla_z f$ (gradient of `f` with respect to `z`), then by chain rule it computes $\\nabla_{var} f = J^{z}_{var} \\nabla_z f$, where $J^{z}_{var}$ is the Jacobian of `z` with respect to `var`.\n",
        "## Next, `optimizer.step()` call adjusts the variable `var` in the opposite direction of $\\nabla_{var} f.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "NLxQgQaD7Krq"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(var,optimizer,softmax,loss,target,nIter,nPrint):\n",
        "  for i in range(nIter):\n",
        "    z = softmax(var)\n",
        "    f = loss(z-target,1.0)\n",
        "    optimizer.zero_grad()\n",
        "    f.backward()\n",
        "    optimizer.step()\n",
        "    if i%nPrint==0:\n",
        "      with np.printoptions(precision=3, suppress=True):\n",
        "        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5viWaJpSiDuN"
      },
      "source": [
        "# Gradient descent with Huber Loss\n",
        "## The following cell shows how `gradient_descent` function can be used.\n",
        "## The cell first creates a target 1-hot vector `y`, where only the 3rd place is on.\n",
        "## It also creates a variable `x` with random initialization and an optimizer.\n",
        "## Learning rate and momentum has been set to 0.1 and 0.9, respectively.\n",
        "## Then it calls `gradient_descent` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzRgWv_NiIeQ",
        "outputId": "f2e8ec7b-6a72-41dd-cdb8-4823944ca52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.113 0.083 0.008 0.013 0.182 0.185 0.051 0.1   0.114 0.152] Loss: 0.559487\n",
            "Iteration: 100 Variable: [0.121 0.111 0.056 0.018 0.125 0.125 0.081 0.118 0.121 0.124] Loss: 0.500452\n",
            "Iteration: 200 Variable: [0.007 0.007 0.945 0.002 0.007 0.007 0.006 0.007 0.007 0.007] Loss: 0.001688\n",
            "Iteration: 300 Variable: [0.005 0.005 0.958 0.001 0.005 0.005 0.004 0.005 0.005 0.005] Loss: 0.000975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 400 Variable: [0.004 0.004 0.965 0.001 0.004 0.004 0.004 0.004 0.004 0.004] Loss: 0.000688\n",
            "Iteration: 500 Variable: [0.004 0.004 0.969 0.001 0.004 0.004 0.003 0.004 0.004 0.004] Loss: 0.000531\n",
            "Iteration: 600 Variable: [0.003 0.003 0.972 0.001 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000432\n",
            "Iteration: 700 Variable: [0.003 0.003 0.974 0.001 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000364\n",
            "Iteration: 800 Variable: [0.003 0.003 0.976 0.001 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000315\n",
            "Iteration: 900 Variable: [0.003 0.003 0.978 0.001 0.003 0.003 0.002 0.003 0.003 0.003] Loss: 0.000277\n"
          ]
        }
      ],
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target 1-hot vector:\",y.numpy())\n",
        "x = torch.randn(y.shape,requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,Huber_Loss,y,1000,100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS9rVoxENEaH"
      },
      "source": [
        "# Let's do a practice on writing differentiable layer using Pytorch's function template\n",
        "\n",
        "## The cell below implements the following function (squared L2 norm)\n",
        "\n",
        "$f(x) = ||x||^2$\n",
        "\n",
        "## Note that x is a vector or a tensor, and f(x) is simply the sum of the square of the elements of x\n",
        "\n",
        "## The Jacobian of f(x) with respect to x is:\n",
        "\n",
        "$J_{x}^{f} = 2x$\n",
        "\n",
        "## The Jacobian here is actually the gradient of $f(x)$ because $f(x)$ outputs a scalar (i.e., single) value\n",
        "\n",
        "## The backward pass implements: <font color='red'>input_grad = Jacobian x output_grad</font> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "7dANLmUENEaH",
        "outputId": "c989248d-ab05-4c6a-c5c1-2a783703c8e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When x is tensor([ 3., -1.,  0.,  1.]) function value is 11.0\n",
            "Gradient of f at x: tensor([ 6., -2.,  0.,  2.])\n"
          ]
        }
      ],
      "source": [
        "# Inherit from torch.autograd.Function\n",
        "class My_f(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        f = torch.sum(x**2)\n",
        "        ctx.save_for_backward(x,torch.tensor(2.0)) # note that the constant 2.0 is cast as a pytorch tensor before saving\n",
        "        return f\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        x,two = ctx.saved_tensors\n",
        "        # Return Jacobian-vector product (chain rule)\n",
        "        input_grad = two*x*output_grad\n",
        "                \n",
        "        return input_grad\n",
        "\n",
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "my_fval = My_f.apply(x)\n",
        "print(\"When x is\", x.data, \"function value is\",my_fval.item())\n",
        "\n",
        "# compute gradient of f at x\n",
        "g = torch.autograd.grad(my_fval,x)[0]\n",
        "\n",
        "print(\"Gradient of f at x:\",g.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtIf2LRqvOph"
      },
      "source": [
        "# <font color='red'>20% Weight:</font> In this markdown cell, using [math mode](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html), write gradient of Huber loss function: $output = \\sum_i 0.5 m_i (input)^{2}_{i} + \\delta (1-m_i)(|input_i|-0.5 \\delta)$ with respect to $input.$ Treat $m_i$ to be independent of $input_i,$ because we replaced *if* control statement with $m_i.$\n",
        "## Your solution : $\\frac{\\partial (output)}{\\partial (input)_i} =  m_i (input_i) + \\delta(1-m_i)*sign(input_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8SaBQ-lXbg"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> Define your own (correct!) rule of differentiation for Huber loss function\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## For this function,  chain rule (Jacobian-vector product) takes the following form: $\\frac{\\partial (loss)}{\\partial (input)_i} = \\frac{\\partial (output)}{\\partial (input)_i} \\frac{\\partial (loss)}{\\partial (output)}.$\n",
        "# In the `backward` method below, $\\frac{\\partial (loss)}{\\partial (output)}$ is denoted by `output_grad` and the $i^{th}$ component of `input_grad` is symbolized by $\\frac{\\partial (loss)}{\\partial (input)_i}.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UX4zC76XlWr0"
      },
      "outputs": [],
      "source": [
        "# Inherit from torch.autograd.Function\n",
        "class My_Huber_Loss(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, delta):\n",
        "        m = (torch.abs(input)<=delta).float()\n",
        "        ctx.save_for_backward(input,torch.tensor(m),torch.tensor(delta))\n",
        "        output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        input, m, delta = ctx.saved_tensors\n",
        "\n",
        "        # Return Jacobian-vector product (chain rule)\n",
        "        # For Huber loss function the Jacobian happens to be a diagonal matrix\n",
        "        # Also, note that output_grad is a scalar, because forward function returns a scalar value\n",
        "        input_grad = (m * input + (delta * (1-m))* torch.sign(input)) * output_grad # complete this line, do not use for loop\n",
        "        # must return two gradients becuase forward function takes in two arguments\n",
        "        return input_grad, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkG5zXGZcgja"
      },
      "source": [
        "# Gradient Descent on Your Own Huber Loss\n",
        "## You should get almost identical results as before if your rule of differentation is correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "6DKnFDK0pPjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b0bcf4-abb9-4ef4-a883-53656d900b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.035 0.115 0.023 0.023 0.032 0.072 0.1   0.057 0.335 0.205] Loss: 0.571437\n",
            "Iteration: 100 Variable: [0.005 0.009 0.936 0.004 0.005 0.008 0.009 0.007 0.009 0.009] Loss: 0.002280\n",
            "Iteration: 200 Variable: [0.004 0.006 0.956 0.003 0.003 0.006 0.006 0.005 0.006 0.006] Loss: 0.001091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 300 Variable: [0.003 0.005 0.964 0.002 0.003 0.005 0.005 0.004 0.005 0.005] Loss: 0.000743\n",
            "Iteration: 400 Variable: [0.003 0.004 0.968 0.002 0.003 0.004 0.004 0.004 0.004 0.004] Loss: 0.000564\n",
            "Iteration: 500 Variable: [0.002 0.004 0.972 0.002 0.002 0.004 0.004 0.003 0.004 0.004] Loss: 0.000454\n",
            "Iteration: 600 Variable: [0.002 0.004 0.974 0.002 0.002 0.003 0.003 0.003 0.003 0.003] Loss: 0.000380\n",
            "Iteration: 700 Variable: [0.002 0.003 0.976 0.001 0.002 0.003 0.003 0.003 0.003 0.003] Loss: 0.000326\n",
            "Iteration: 800 Variable: [0.002 0.003 0.977 0.001 0.002 0.003 0.003 0.003 0.003 0.003] Loss: 0.000286\n",
            "Iteration: 900 Variable: [0.002 0.003 0.979 0.001 0.002 0.003 0.003 0.002 0.003 0.003] Loss: 0.000254\n"
          ]
        }
      ],
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target:\",y.numpy())\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,My_Huber_Loss.apply,y,1000,100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHS-3lzkNEaI"
      },
      "source": [
        "# <font color='red'>20% Weight:</font> In this markdown using math mode write Jacobian of softmax function: $(output)_i = \\frac{exp((input)_i)}{ \\sum_j exp((input)_j)}.$\n",
        "## Your solution : \n",
        "\\begin{equation*}\n",
        "    \\frac{\\partial (output)_j}{\\partial (input)_i} = \\begin{cases}\n",
        "               {output_i*(1-output_j)},               & i = j,\\\\\n",
        "               -{output_i*output_j} , & \\text{otherwise.}\n",
        "           \\end{cases}\n",
        "\\end{equation*}\n",
        "# Putting them together:\n",
        "\\begin{equation*}\n",
        "\\frac{\\partial (output)_j}{\\partial (input)_i} = output_i(1(i=j) - output_j(i!= j)) \n",
        "\\end{equation*}\n",
        "\\begin{equation*} \n",
        "  or, \\frac{\\partial (output)_j}{\\partial (input)_i} = output_i(i=j) - output_i * output_j (i != j)\n",
        "\\end{equation*}\n",
        "\\begin{equation*} \n",
        "  Therefore, \\frac{\\partial (output)_j}{\\partial (input)_i}= diagonal(output) - outerproduct(outer, outer) \n",
        "\\end{equation*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K4Q7ePPdfy"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> Your own softmax with forward and backward functions\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## The Jacobian-vector product (chain rule) takes the following form using summation sign: $\\frac{\\partial (loss)}{\\partial (input)_i} = \\sum_j \\frac{\\partial (output)_j}{\\partial (input)_i} \\frac{\\partial (loss)}{\\partial (output)_j}$\n",
        "# Once again note that, in the `backward` method below, $i^{th}$ component of `input_grad` and $j^{th}$ component of `output_grad` are denoted by $\\frac{\\partial (loss)}{\\partial (input)_i}$ and $\\frac{\\partial (loss)}{\\partial (output)_j}$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zn52-xK_PijV"
      },
      "outputs": [],
      "source": [
        "# Inherit from Function\n",
        "class My_softmax(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = F.softmax(input,dim=0)\n",
        "        ctx.save_for_backward(output) # this is the only tensor you will need to save for backward function\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        output = ctx.saved_tensors[0]\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        # return Jacobian-vecor product\n",
        "        input_grad = torch.sum((torch.diag(output) - torch.outer(output, output)) * output_grad,1)  # Complete this line\n",
        "        return input_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcixVFs4cwHO"
      },
      "source": [
        "# Gradient Descent on your own Huber Loss and your own softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UejqQeb4RZk0",
        "outputId": "ce2507e5-c275-431d-c41c-35de18fd435a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([-1.2793, -1.2194, -0.4673, -2.5135, -0.3498,  0.9110,  0.4048,  0.5329,\n",
            "         0.7712,  0.2621], requires_grad=True)\n",
            "Iteration: 0 Variable: [0.025 0.027 0.056 0.007 0.063 0.223 0.135 0.153 0.194 0.117] Loss: 0.519346\n",
            "Iteration: 100 Variable: [0.003 0.003 0.947 0.001 0.006 0.008 0.008 0.008 0.008 0.008] Loss: 0.001618\n",
            "Iteration: 200 Variable: [0.002 0.002 0.959 0.001 0.005 0.006 0.006 0.006 0.006 0.006] Loss: 0.000943\n",
            "Iteration: 300 Variable: [0.002 0.002 0.966 0.001 0.004 0.005 0.005 0.005 0.005 0.005] Loss: 0.000668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 400 Variable: [0.002 0.002 0.97  0.001 0.003 0.005 0.005 0.005 0.005 0.004] Loss: 0.000517\n",
            "Iteration: 500 Variable: [0.002 0.002 0.973 0.    0.003 0.004 0.004 0.004 0.004 0.004] Loss: 0.000422\n",
            "Iteration: 600 Variable: [0.001 0.002 0.975 0.    0.003 0.004 0.004 0.004 0.004 0.004] Loss: 0.000356\n",
            "Iteration: 700 Variable: [0.001 0.001 0.977 0.    0.003 0.004 0.003 0.004 0.004 0.003] Loss: 0.000308\n",
            "Iteration: 800 Variable: [0.001 0.001 0.978 0.    0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000271\n",
            "Iteration: 900 Variable: [0.001 0.001 0.979 0.    0.002 0.003 0.003 0.003 0.003 0.003] Loss: 0.000242\n"
          ]
        }
      ],
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(y)\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,My_softmax.apply,My_Huber_Loss.apply,y,1000,100)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}